{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0    2.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1    9.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2    6.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     5.0   \n",
       "3    0.0     0.0     0.0     0.0     1.0     2.0     0.0     0.0     0.0   \n",
       "4    3.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0     0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "1     0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "2     0.0    ...          0.0       0.0       0.0      30.0      43.0   \n",
       "3     0.0    ...          3.0       0.0       0.0       0.0       0.0   \n",
       "4     0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0       0.0       0.0       0.0       0.0       0.0  \n",
       "1       0.0       0.0       0.0       0.0       0.0  \n",
       "2       0.0       0.0       0.0       0.0       0.0  \n",
       "3       1.0       0.0       0.0       0.0       0.0  \n",
       "4       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "csv_path = 'Apparel/apparel-trainval.csv'#raw_input(\"Enter path to input CSV file: \")\n",
    "dataset = pd.read_csv(csv_path)\n",
    "# dataset = dataset.truncate(before=0, after=55000)\n",
    "dataset = dataset.astype('float64')\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "label\n"
     ]
    }
   ],
   "source": [
    "#We inititalize X and Y as the attributes and label respectively for the decision tree\n",
    "Attributes = dataset.keys()[1:]\n",
    "Label = dataset.keys()[0]\n",
    "print(len(Attributes))\n",
    "print(Label)\n",
    "\n",
    "mean = {}\n",
    "std = {}\n",
    "for x in Attributes:\n",
    "    mean[x] = np.mean(dataset[x])\n",
    "    std[x] = np.std(dataset[x])\n",
    "    \n",
    "normalized_dataset = dataset\n",
    "for x in Attributes:\n",
    "    normalized_dataset[x] = (normalized_dataset[x] - mean[x]) / std[x]\n",
    "    \n",
    "#split data into train data and validation data\n",
    "splitted = np.split(normalized_dataset, [int(.8 * len(dataset))])\n",
    "train_data = splitted[0]\n",
    "validation_data = splitted[1]\n",
    "    \n",
    "X = train_data[Attributes].values\n",
    "# X[X != X] = 0\n",
    "assert len(X[X != X]) == 0 # No nan should be present\n",
    "Y = train_data[Label].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "#     print labels.shape\n",
    "    num_labels = len(labels)\n",
    "    unique = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((num_labels, unique))\n",
    "    one_hot_encode[np.arange(num_labels), labels] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self, s1, s2, s3, s4_list, hidden_act, output_act, n):\n",
    "        # Define Hyperparameters\n",
    "        self.inputLayerSize = s1\n",
    "        self.outputLayerSize = s2\n",
    "        self.hiddenLayerCount = s3\n",
    "        self.hiddenLayerSizes = s4_list  # s4_list = [hiddenLayer1Size, hiddenLayer2Size, ...]\n",
    "        self.hiddenActivation = hidden_act  # 'sigmoid', 'relu' or 'tanh'\n",
    "        self.outputActivation = output_act  # 'sigmoid', 'relu' or 'tanh'\n",
    "        self.learningRate = n\n",
    "        \n",
    "        # Weights (parameters)\n",
    "        self.W = []\n",
    "        self.W.append( np.random.rand(self.inputLayerSize, self.hiddenLayerSizes[0]) )\n",
    "        for i in range(self.hiddenLayerCount-1):\n",
    "            self.W.append( np.random.rand(self.hiddenLayerSizes[i], self.hiddenLayerSizes[i+1]) )\n",
    "        self.W.append( np.random.rand(self.hiddenLayerSizes[self.hiddenLayerCount-1], self.outputLayerSize) )\n",
    "    \n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] = self.W[i] * math.sqrt(2.0/s1)\n",
    "            \n",
    "        self.b = [None]*(self.hiddenLayerCount+1)\n",
    "        for i in range(len(self.b)):\n",
    "            self.b[i] = np.random.rand()\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Propagate inputs through network\n",
    "        self.z = []  # z2, z3, ... z(n+1) n=hiddenLayerCount\n",
    "        self.a = []  # a2, a3, ... a(n+1)\n",
    "        \n",
    "        self.z.append( np.dot(x, self.W[0]) )\n",
    "        \n",
    "        for i in range(self.hiddenLayerCount):\n",
    "            cur_a = self.activations[self.hiddenActivation]( self.z[i] )\n",
    "            self.a.append( cur_a )\n",
    "            self.z.append( np.dot(cur_a, self.W[i+1]) )\n",
    "#             print(self.a[i].shape)\n",
    "\n",
    "#         print self.z[self.hiddenLayerCount].shape\n",
    "        yHat = self.activations[self.outputActivation]( self.z[self.hiddenLayerCount] )\n",
    "        \n",
    "        \n",
    "#         zz = []\n",
    "#         aa = []\n",
    "#         w = self.W[0]+0.00001\n",
    "#         zz.append( np.dot(x, w) )\n",
    "#         for i in range(self.hiddenLayerCount):\n",
    "#             cur_a = self.activations[self.hiddenActivation]( zz[i] )\n",
    "#             aa.append( cur_a )\n",
    "#             zz.append( np.dot(cur_a, self.W[i+1]) )\n",
    "# #             print(self.a[i].shape)\n",
    "# #             print(self.z[i+1].shape)\n",
    "#         yHat_test = self.activations[self.outputActivation]( zz[self.hiddenLayerCount] )\n",
    "#         j = 0.5 * (self.y - yHat_test)**2\n",
    "#         print self.y.shape, yHat_test.shape, j.shape\n",
    "        \n",
    "#         zz = []\n",
    "#         aa = []\n",
    "#         w = self.W[0]-0.00001\n",
    "#         zz.append( np.dot(x, w) )\n",
    "#         for i in range(self.hiddenLayerCount):\n",
    "#             cur_a = self.activations[self.hiddenActivation]( zz[i] )\n",
    "#             aa.append( cur_a )\n",
    "#             zz.append( np.dot(cur_a, self.W[i+1]) )\n",
    "# #             print(self.a[i].shape)\n",
    "# #             print(self.z[i+1].shape)\n",
    "#         yHat_test = self.activations[self.outputActivation]( zz[self.hiddenLayerCount] )\n",
    "#         j = j - (  0.5 * (self.y - yHat_test)**2  )\n",
    "        \n",
    "#         print 'estimated', (j/0.00002).shape\n",
    "        \n",
    "        return yHat\n",
    "    \n",
    "    \n",
    "    def backward2(self, x, y, yHat):\n",
    "        dJdW = [None] * len(self.W)\n",
    "#         print len(self.W), len(self.a)\n",
    "        \n",
    "        a_idx = len(self.a)-1\n",
    "        z_idx = len(self.z)-1\n",
    "#         print(y.shape)\n",
    "        df_z = self.derivatives['d_'+self.outputActivation]( self.z[z_idx] )\n",
    "    \n",
    "        if self.outputLayerSize == 1:\n",
    "            y = y.reshape(len(y),1)\n",
    "        \n",
    "        delta = np.multiply( -(y - yHat), df_z )\n",
    "        dJdW[ len(dJdW)-1 ] = np.dot( self.a[a_idx].T, delta )\n",
    "        \n",
    "        a_idx -= 1\n",
    "        z_idx -= 1\n",
    "#         print(\"-----\")\n",
    "        for i in range(len(dJdW)-2, 0, -1):\n",
    "            df_z = self.derivatives['d_'+self.hiddenActivation]( self.z[z_idx] )\n",
    "            print delta.shape, self.W[i+1].shape, df_z.shape\n",
    "#             print(delta.shape, self.W[i+1].T.shape)\n",
    "            delta = np.dot(delta, self.W[i+1].T) * df_z\n",
    "            dJdW[i] = np.dot( self.a[a_idx].T, delta )\n",
    "#             print(delta.shape)\n",
    "            a_idx -= 1\n",
    "            z_idx -= 1\n",
    "        \n",
    "        df_z = self.derivatives['d_'+self.hiddenActivation]( self.z[z_idx] )\n",
    "        delta = np.dot(delta, self.W[1].T) * df_z\n",
    "        dJdW[0] = np.dot( x.T, delta )\n",
    "#         print \"actual\", dJdW[0].shape\n",
    "        \n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] = self.W[i] - self.learningRate * dJdW[i]\n",
    "            \n",
    "            \n",
    "    def backward(self, x, y, yHat):\n",
    "        dJdW = [None] * len(self.W)\n",
    "        dJdb = [None] * len(self.b)\n",
    "#         print len(self.W), len(self.a)\n",
    "        \n",
    "        a_idx = len(self.a)-1\n",
    "        z_idx = len(self.z)-1\n",
    "#         print(y.shape)\n",
    "#         df_z = self.derivatives['d_'+self.outputActivation]( self.z[z_idx] )\n",
    "    \n",
    "        if self.outputLayerSize == 1:\n",
    "            y = y.reshape(len(y),1)\n",
    "        \n",
    "        delta = (yHat-y)\n",
    "        dJdW[ len(dJdW)-1 ] = np.dot( self.a[a_idx].T, delta )\n",
    "        \n",
    "        a_idx -= 1\n",
    "        z_idx -= 1\n",
    "#         print(\"-----\")\n",
    "        for i in range(len(dJdW)-2, 0, -1):\n",
    "            df_z = self.derivatives['d_'+self.hiddenActivation]( self.z[z_idx] )\n",
    "            delta = np.multiply( np.dot(delta, self.W[i+1].T), df_z )\n",
    "            dJdW[i] = np.dot( self.a[a_idx].T, delta )\n",
    "#             print(delta.shape)\n",
    "            a_idx -= 1\n",
    "            z_idx -= 1\n",
    "        \n",
    "        df_z = self.derivatives['d_'+self.hiddenActivation]( self.z[z_idx] )\n",
    "        delta = np.multiply( np.dot(delta, self.W[1].T), df_z )\n",
    "        dJdW[0] = np.dot( x.T, delta )\n",
    "#         print \"actual\", dJdW[0].shape\n",
    "        \n",
    "        for i in range(len(self.W)):\n",
    "#             print(dJdW[i])\n",
    "            self.W[i] = self.W[i] - self.learningRate * dJdW[i]\n",
    "            \n",
    "            \n",
    "    def fit(self, x, y, num_ephocs=5, batch_size=2000):\n",
    "        self.y = y\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        for i in range(num_ephocs):\n",
    "            while end < x.shape[0]:\n",
    "                x_batch = x[start:end,:]\n",
    "                yHat = self.forward(x_batch)\n",
    "                self.backward(x_batch, y[start:end,:], yHat)\n",
    "                start += batch_size\n",
    "                end += batch_size\n",
    "            x_batch = x[start:x.shape[0],:]\n",
    "            yHat = self.forward(x_batch)\n",
    "            self.backward(x_batch, y[start:x.shape[0],:], yHat)\n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.forward(x)\n",
    "        \n",
    "    \n",
    "    def sigmoid(z):\n",
    "        return 1.0 / (1 + np.exp(-z))\n",
    "    \n",
    "    def relu(z):\n",
    "        ret = np.maximum(0,z)\n",
    "        return ret\n",
    "    \n",
    "    def tanh(z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def softmax(z):\n",
    "#         print \"<---softmax\"\n",
    "        e_z = np.exp(z - z.max(1).reshape(z.shape[0],1))\n",
    "#         print \"softmax--->\"\n",
    "        ret = e_z / np.sum(e_z,1).reshape(z.shape[0],1)\n",
    "#         print 'ret', np.sum(ret)\n",
    "        return ret\n",
    "    \n",
    "    def d_sigmoid(z):\n",
    "        sigmoid_z = 1.0 / (1 + np.exp(-z))\n",
    "        return sigmoid_z*(1-sigmoid_z)\n",
    "    \n",
    "    def d_relu(z):\n",
    "        ret = copy.deepcopy(z)\n",
    "        ret[ret <= 0] = 0\n",
    "        ret[ret > 0] = 1\n",
    "        return ret\n",
    "    \n",
    "    def d_tanh(z):\n",
    "        tanh_z = np.tanh(z)\n",
    "        return 1 - (tanh_z)**2\n",
    "    \n",
    "    def d_softmax(z):\n",
    "        e_z = np.exp(z - z.max(1).reshape(z.shape[0],1))\n",
    "        sm = e_z / np.sum(e_z,1).reshape(z.shape[0],1)\n",
    "#         sm = sm.reshape(-1,1)\n",
    "#         return np.diagflat(sm) - np.dot(sm, sm.T)\n",
    "#         e_z = np.exp(z-np.amax(z))\n",
    "#         sm = e_z / np.sum(e_z)\n",
    "        ret = sm*(1-sm)\n",
    "        return ret\n",
    "    \n",
    "    activations ={'sigmoid': sigmoid,\n",
    "                 'relu': relu,\n",
    "                 'tanh': tanh,\n",
    "                 'softmax': softmax\n",
    "                 }\n",
    "    derivatives ={'d_sigmoid': d_sigmoid,\n",
    "                 'd_relu': d_relu,\n",
    "                 'd_tanh': d_tanh,\n",
    "                 'd_softmax': d_softmax\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.816083333333\n"
     ]
    }
   ],
   "source": [
    "nn = Neural_Network(784,10,1,[128],'tanh','softmax', 0.01)\n",
    "# print(nn.W)\n",
    "# X = np.array([ [3,5], [1,10], [7,2] ])\n",
    "# Y = np.array([0,1,1])\n",
    "# print X.shape\n",
    "# print X\n",
    "# y = nn.forward(X)\n",
    "# print y\n",
    "# nn.backward(X, Y, y)\n",
    "# print nn.W\n",
    "# print Y.shape\n",
    "encoder = LabelEncoder()\n",
    "Y = train_data[Label].values\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n",
    "Y = one_hot_encode(Y)\n",
    "# print Y\n",
    "nn.fit(X, Y, num_ephocs=100, batch_size=100)\n",
    "# print(nn.W)\n",
    "op = nn.predict(validation_data[Attributes].values)\n",
    "\n",
    "Y = validation_data[Label].values\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n",
    "Y = one_hot_encode(Y)\n",
    "T, F = 0, 0\n",
    "for i in range(op.shape[0]):\n",
    "#     print np.argmax(op[i,:])\n",
    "#     print op[i]\n",
    "#     print np.argmax(Y[i])\n",
    "#     print \"\"\n",
    "    if np.argmax(Y[i]) == np.argmax(op[i,:]):\n",
    "        T += 1\n",
    "    else:\n",
    "        F += 1\n",
    "print \"Accuracy = \", float(T)/(T+F)\n",
    "#     print\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array( [ [1,9,2], [3,4,5], [10,9,8] ] )\n",
    "y = np.array( [1,2,3] ).reshape(3,1)\n",
    "x-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.49652977 -0.22665002  0.59535449]\n",
      " [-0.89489335  0.33459019  0.50173344]\n",
      " [-0.79411626  0.20124216 -0.97674158]]\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [0. 1. 0.]]\n",
      "[[-0.49652977 -0.22665002  0.59535449]\n",
      " [-0.89489335  0.33459019  0.50173344]\n",
      " [-0.79411626  0.20124216 -0.97674158]]\n"
     ]
    }
   ],
   "source": [
    "def reluDerivative(x):\n",
    "    y=copy.deepcopy(x)\n",
    "    y[y<=0] = 0\n",
    "    y[y>0] = 1\n",
    "    return y\n",
    "\n",
    "z = np.random.uniform(-1, 1, (3,3))\n",
    "print z\n",
    "x = reluDerivative(z)\n",
    "print x\n",
    "print z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
